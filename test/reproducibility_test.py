import filecmp
import logging
import subprocess
import yaml

dict_file_types = ["json", "yml", "yaml"]
logger = logging.getLogger(__name__)


def reproducibility_tests(args=None, config_path=None):
    """Runs commands in config file and compares the generated files to those that are expected."""

    if args is not None:
        config_path = args.config

    with open(config_path, "r") as f:
        modules = yaml.load(f, Loader=yaml.FullLoader)

    all_passed = True
    for module in modules:
        # log the path for test outcome and expected outcome of the module
        conf = modules[module]
        # compare whether csv files generated by the model pipeline is the same as the expected files
        # located in test/true folder
        true_dir, test_dir = conf["true_dir"], conf["test_dir"]
        files_to_compare = [f for f in conf["files_to_compare"] if f.split('.')[-1] not in dict_file_types]
        match, mismatch, errors = filecmp.cmpfiles(true_dir, test_dir, files_to_compare, shallow=True)
        # if there is a mismatch or no file is match, reproducibility test is failed
        if len(mismatch) > 0 or len(match) == 0:
            logger.error(
                "{} file(s) do(es) not match, reproducibility test of model pipeline step {}': FAILED".format(mismatch,
                                                                                                              module))
            all_passed = False
        else:
            logger.info("Reproducibility test of model pipeline stage {}: PASSED".format(module))

    if all_passed:
        logger.info("Success, all reproducibility tests passed!")
